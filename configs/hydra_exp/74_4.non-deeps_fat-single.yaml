# @package _global_

# close to exp74
# but with single conv without groups, width is same.
# different from 74_1, because only one conv
# different from 74_2, because same width

# upd. after some epochs working very close to 74_1 while being faster
# conclusion: depth at fist convs is more important than width, but because 2 convs works ~ same as one, it maybe saturates quickly
# taking into account that in first experiment i had grouped convs it leads to question - maybe RegNet has so many first layers only because they use groups? 

# [11-13 00:04:41] - Epoch 90 | lr 7.33e-05
# [11-13 00:13:56] - Train loss: 2.8642 | Acc@1: 68.2573 | Acc@5: 85.6770
# [11-13 00:13:56] - Val   loss: 1.9643 | Acc@1: 74.7300 | Acc@5: 92.1040
# [11-13 00:13:56] - Model params: 23.04M
# [11-13 00:13:56] - Acc@1 74.730 Acc@5 92.104
# [11-13 00:13:56] - Total time: 14h 26.7m

# close to exp74 but much faster. leaving this experiment as baseline for the future 

defaults:
  - /base@_here_
  
log:
  exp_name: non-deeps_fat-head-same-width-single

  histogram: True
  print_model: True

model:
  _target_: sota_imagenet.model.CModel
  extra_kwargs:
    NonDeepBlock:
      norm: nn.BatchNorm2d
      scaled: True

  layer_config:
    - [-1, 1, pt.modules.SpaceToDepth, 4] # os=4
    # residual starts from the very first layer
    - [-1, 1, NonDeepBlock, [48, 128]]
    - [-1, 1, NonDeepBlock, [128, 128]]
    - [-1, 1, nn.AvgPool2d, [2, 2]] # os=8
    - [-1, 1, NonDeepBlock, [128, 256]]
    - [-1, 2, NonDeepBlock, [256, 256]]
    - [-1, 1, nn.AvgPool2d, [2, 2]] # os=16
    - [-1, 1, NonDeepBlock, [256, 384]] # {groups_width: 64}
    - [-1, 8, NonDeepBlock, [384, 384]] # {groups_width: 64}
    - [-1, 1, pt.modules.FastGlobalAvgPool2d, [], {flatten: True}]
    - [-1, 1, nn.Linear, [384, 2048]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.Linear, [2048, 2048]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.Linear, [2048, 1000]]

# using default SGD
optim:
  _target_: torch.optim._multi_tensor.SGD
  momentum: 0.9
  weight_decay: 3e-5
  lr: 0

# optim:
#   # in PyTorch implementation weight decay is lr * wd, so 0.1 == 1e-4 as in AdamW paper
#   _target_: src.optimizers.AdamLayerwise
#   weight_decay: 1e-2
#   betas: [0.9, 0.995]
#   lr: 0

# use label smoothing
criterion:
  smoothing: 0.1

loader:
  image_size: 224
  batch_size: 224
  blur_prob: 0.2
  gray_prob: 0.2
  color_twist_prob: 0.4
  re_prob: 0.3

val_loader:
  image_size: 288 # to test on the same resolution as timm

run:
  stages:
    # SGD requires warmup
    - {start: 0, end: 3, lr: [0.001, 0.1]}
    - {start: 3, end: 90, lr: [0.1, 0], lr_mode: cos}

  extra_callbacks:
    - _target_: sota_imagenet.callbacks.CutmixMixup
      cutmix_alpha: 1.0
      mixup_alpha: 0.2
      prob: 1.0

filter_from_wd: [gain] # filter bias and gain from wd
init_gamma: 1.7 # larger value to compensate for variance reduction after BlurPool
