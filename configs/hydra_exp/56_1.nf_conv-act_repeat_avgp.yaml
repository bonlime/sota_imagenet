# @package _global_

# same as exp56 but using AvgPool2d(2, 2) instead of BlurPool. It avoid padding and should be faster
# expected that it would give almost the same results

# trained only 65/90 epochs but results are very close
# BlurPool
# [10-20 12:09:17] - Epoch 65 | lr 5.97e-02
# [10-20 12:27:41] - Train loss: 3.1068 | Acc@1: 59.8983 | Acc@5: 80.8834
# [10-20 12:27:41] - Val   loss: 1.9702 | Acc@1: 73.9740 | Acc@5: 91.9380
# avgpool 2x2
# [10-22 13:44:48] - Epoch 65 | lr 5.97e-02
# [10-22 14:04:45] - Train loss: 3.1065 | Acc@1: 60.1632 | Acc@5: 81.0367
# [10-22 14:04:45] - Val   loss: 1.9697 | Acc@1: 73.5160 | Acc@5: 91.8900


defaults:
  - /base@_here_
  
log:
  exp_name: nf-conv-act_repeat
  histogram: True
  print_model: True

model:
  _target_: sota_imagenet.model.CModel
  extra_kwargs:
    ConvActBlock:
      activation: "'swish_hard'"
      conv_kwargs:
        gamma: "${init_gamma}"
        gain_init: 0.1
        n_heads: 1
    NormFreeBlockTimm:
      activation: "'swish_hard'"
      groups_width: 64
      alpha: 0.2
      conv_kwargs:
        gamma: "${init_gamma}"
      attention_type: "'eca9'"
      keep_prob: 0.85 # will set the same for each block and then patch to linearly increase
      regnet_attention: True
    VarEMA:
      use: False
    scaled_conv3x3: {gamma: "${init_gamma}"}
    torch.nn.SiLU: {inplace: True}

  layer_config:
    # residual starts from the very firt layer. 
    # larger gain during first channel increase for faster convergence during training
    - [-1, 1, ConvActBlock, [3, 16], {stride: 2, conv_kwargs: {gain_init: 1.0}}]
    - [-1, 1, ConvActBlock, [16, 32], {conv_kwargs: {gain_init: 0.5}}]
    - [-1, 1, ConvActBlock, [32, 64], {conv_kwargs: {gain_init: 0.5}}]
    - [-1, 1, VarEMA, 64]
    - [-1, 1, ConvActBlock, [64, 64], {stride: 2}]
    - [-1, 1, VarEMA, 64]
      # stage 1
    - [-1, 4, "ConvActBlock", [64, 64]]
      # stage 2
    - [-1, 1, VarEMA, 64]
    - [-1, 1, "ConvActBlock", [64, 128], {stride: 2}]
    - [-1, 1, VarEMA, 128]
    - [-1, 5, "ConvActBlock", [128, 128], {groups_width: 64}]
      # stage 3
    - [-1, 1, VarEMA, 128]
    # - [-1, 1, pt.modules.BlurPool, 128]
    - [-1, 1, nn.AvgPool2d, [2, 2]]
    - [-1, 1, VarEMA, 128]
    - [-1, 1, NormFreeBlockTimm, [128, 768, 384]]
    - [-1, 6, NormFreeBlockTimm, [768, 768, 384]]
    - [-1, 1, VarEMA, 768]
      # stage 4
    # - [-1, 1, pt.modules.BlurPool, 768]
    - [-1, 1, nn.AvgPool2d, [2, 2]]
    - [-1, 1, VarEMA, 768]
    - [-1, 5, NormFreeBlockTimm, [768, 768, 384]]
      # head
    - [-1, 1, scaled_conv1x1, [768, 2304], {gamma: "${init_gamma}"}]
    - [-1, 1, torch.nn.SiLU]
    - [-1, 1, pt.modules.FastGlobalAvgPool2d, [], {flatten: True}]
    - [-1, 1, torch.nn.Dropout, [0.2]]
    - [-1, 1, nn.Linear, [2304, 1000]]

# using default SGD
optim:
  _target_: torch.optim._multi_tensor.SGD
  momentum: 0.9
  weight_decay: 3e-5
  lr: 0

# optim:
#   # in PyTorch implementation weight decay is lr * wd, so 0.1 == 1e-4 as in AdamW paper
#   _target_: src.optimizers.AdamLayerwise
#   weight_decay: 1e-2
#   betas: [0.9, 0.995]
#   lr: 0

# use label smoothing
criterion:
  smoothing: 0.1

loader:
  image_size: 224
  batch_size: 224
  blur_prob: 0.2
  gray_prob: 0.2
  color_twist_prob: 0.4
  re_prob: 0.3

val_loader:
  image_size: 288 # to test on the same resolution as timm

run:
  stages:
    # SGD requires warmup
    - {start: 0, end: 90, lr: [0.3, 0], lr_mode: cos}

  extra_callbacks:
    - _target_: sota_imagenet.callbacks.CutmixMixup
      cutmix_alpha: 1.0
      mixup_alpha: 0.2
      prob: 1.0

filter_from_wd: [gain] # filter bias and gain from wd
init_gamma: 2 # larger value to compensate for variance reduction after BlurPool
