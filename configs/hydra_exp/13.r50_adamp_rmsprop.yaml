# @package _global_

# close to exp11 but with AdamP instead of AdamW

defaults:
  - /base@_here_

model:
  _target_: pytorch_tools.models.resnet50

log:
  exp_name: r50_adamp_rmsprop-like
  histogram: True
  save_optim: True # want to see what happens in optimizer close to the end


optim:
  _target_: pytorch_tools.optim.adamp.AdamP
  weight_decay: 3e-4 # wd * lr ~= 1e-5. this is upper bound, could be slightly lower
  eps: 1e-3 # very large eps as in RMSProp config
  lr: 0

criterion:
  smoothing: 0.1

loader:
  image_size: 224
  batch_size: 256

run:
  stages:
    - {start: 0, end: 8, lr: [0, 0.03]}
    - {start: 8, end: 90, lr: [0.03, 0], lr_mode: cos}

init_gamma: 1.7 # proper weight init
