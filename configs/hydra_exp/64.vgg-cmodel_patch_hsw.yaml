# @package _global_

# VGG-like using CModel constructor
# + GAP like in exp62
# + space2depth (block size = 8) like in ConvMixer
#   + first conv: conv3x3(3, 64) -> conv1x1(192, 64)
#   + first 3 maxpools are removed
# + hard-swish activation

# exp63. failed, hypothesis is that ReLU doesn't allow learning the proper first convs from Space2Depth, so add Hswish
# upd. this exp also silently fails due to very large increase in BN running var stats

# run3: 
# move BN after activation. no justification why this may work, just trying
# ... may be promising
# ... works better at start, then huge var kills the progress


# run4
# scaled_conv -> act -> BN. like NormFree only with BN

# run5
# using my custom FRN instead
# using mean B, ., H, W - works fine, using mean ., ., H, W - somehow works but worse. 
# maybe removing first couple of norms all together would help? if NFNet can, why we can't 

# run6
# found bug. was adding x2.mean() to running_var instead of x2.mean(dim=0)#
# even with bug fixed trains like crap

# run7
# FRN version 1 but with renorm and first 2 norm layers removed
# hope this would help to counter the problems appearing from removing 

# run8
# change order of channels in S2D (in theory it should even matter, but let's check how it would perform)
# returned first norms
# added partial conv
# lower init_gamma

# I'm getting really crazy because of not working S2D, have literally no fck idea why it wouldn't work

# change FRN implementation to LN -> IN
defaults:
  - /base@_here_
  
log:
  exp_name: vgg-cmodel_patches_hswish_post-FRNv3
  histogram: True
  print_model: True

model:
  _target_: sota_imagenet.model.CModel
  extra_kwargs:
    scaled_conv1x1:
      gamma: ${init_gamma}
      partial_conv: True
    scaled_conv3x3:
      gamma: ${init_gamma}
      partial_conv: True

  layer_config:
    # VGG16: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
    # 1
    - [-1, 1, pt.modules.SpaceToDepth, 8]

    - [-1, 1, scaled_conv1x1, [192, 64]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, FRN, 64] # 3
    # 2
    - [-1, 1, scaled_conv3x3, [64, 64]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, FRN, 64] # 6
    # - [-1, 1, nn.MaxPool2d, [2, 2]] # os=2
    # 3
    - [-1, 1, scaled_conv3x3, [64, 128]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, FRN, 128] # 9
    # 4`
    - [-1, 1, scaled_conv3x3, [128, 128]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, FRN, 128]
    # - [-1, 1, nn.MaxPool2d, [2, 2]] # os=4
    # 5
    - [-1, 1, scaled_conv3x3, [128, 256]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, FRN, 256]
    # 6
    - [-1, 1, scaled_conv3x3, [256, 256]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, FRN, 256]
    # 7
    - [-1, 1, scaled_conv3x3, [256, 256]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, FRN, 256]
    # - [-1, 1, nn.MaxPool2d, [2, 2]] # os=8
    # 8
    - [-1, 1, scaled_conv3x3, [256, 512]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, FRN, 512]
    # 9
    - [-1, 1, scaled_conv3x3, [512, 512]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, FRN, 512]
    # 10
    - [-1, 1, scaled_conv3x3, [512, 512]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, FRN, 512]
    - [-1, 1, nn.MaxPool2d, [2, 2]] # os=16
    # 11
    - [-1, 1, scaled_conv3x3, [512, 512]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, FRN, 512]
    # 12
    - [-1, 1, scaled_conv3x3, [512, 512]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, FRN, 512]
    # 13
    - [-1, 1, scaled_conv3x3, [512, 512]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, FRN, 512]
    
    # in original VGG they have FC(512 * 7 * 7 -> 4096) here but i'm using GAP + FC(512 -> 4096)
    - [-1, 1, pt.modules.FastGlobalAvgPool2d, [], {flatten: True}]
    - [-1, 1, nn.Linear, [512, 4096]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.Linear, [4096, 4096]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.Linear, [4096, 1000]]

# using default SGD
optim:
  _target_: torch.optim._multi_tensor.SGD
  momentum: 0.9
  weight_decay: 3e-5
  lr: 0

# use label smoothing
criterion:
  smoothing: 0.1

loader:
  image_size: 224
  batch_size: 224
  blur_prob: 0.2
  gray_prob: 0.2
  color_twist_prob: 0.4
  re_prob: 0.3

val_loader:
  image_size: 288 # to test on the same resolution as timm

run:
  stages:
    # SGD requires warmup
    - {start: 0, end: 3, lr: [0.001, 0.1]}
    - {start: 3, end: 90, lr: [0.1, 0], lr_mode: cos}

  extra_callbacks:
    - _target_: sota_imagenet.callbacks.CutmixMixup
      cutmix_alpha: 1.0
      mixup_alpha: 0.2
      prob: 1.0

# filter_from_wd: [gain] # filter bias and gain from wd
# leaving gain param to apply proper init for the model
# using slightly lower value to avoid growth of STD. would it help? who knows
init_gamma: 1.5 # value from NFNet paper for SiLU activation
