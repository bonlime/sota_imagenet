# @package _global_

# close to exp24 + my version of self-attention
# idea is to use dynamic self-attention but after GAP. could be viewed as simple XCIT self-attention 

defaults:
  - /base@_here_
  
log:
  exp_name: nf-repvgg_regnet-attn_sevar3
  histogram: True
  print_model: True

model:
  _target_: src.model.CModel
  extra_kwargs:
    ConvActBlock:
      activation: "'swish_hard'"
      conv_kwargs:
        gamma: "${init_gamma}"
        gain_init: 0.1
        n_heads: 1
    NormFreeBlockTimm:
      activation: "'swish_hard'"
      groups_width: 64
      alpha: 0.2
      gamma: "${init_gamma}"
      # attention_type: "'esa'"
      attention_type: "'se-var3'"
      # attention_kwargs:
      #   num_heads: 8
        # qkv_bias: True
      #   use_proj: True
      keep_prob: 0.85 # will set the same for each block and then patch to linearly increase
      regnet_attention: True
    VarEMA:
      use: False
    scaled_conv3x3: {gamma: "${init_gamma}"}
    torch.nn.SiLU: {inplace: True}

  layer_config:
    # residual starts from the very firt layer. 
    # larger gain during first channel increase for faster convergence during training
    - [-1, 1, ConvActBlock, [3, 16], {stride: 2, conv_kwargs: {gain_init: 1.0}}]
    - [-1, 1, ConvActBlock, [16, 32], {conv_kwargs: {gain_init: 0.5}}]
    - [-1, 1, ConvActBlock, [32, 64], {conv_kwargs: {gain_init: 0.5}}]
    - [-1, 1, VarEMA]
    - [-1, 1, ConvActBlock, [64, 64], {stride: 2}]
    - [-1, 1, VarEMA]
      # stage 1
    - [-1, 4, "ConvActBlock", [64, 64]]
      # stage 2
    - [-1, 1, VarEMA]
    - [-1, 1, "ConvActBlock", [64, 128], {stride: 2}]
    - [-1, 1, VarEMA]
    - [-1, 5, "ConvActBlock", [128, 128], {groups_width: 64}]
      # stage 3
    - [-1, 1, VarEMA]
    - [-1, 1, "pt.modules.BlurPool", 128]
    - [-1, 1, VarEMA]
    - [-1, 1, "NormFreeBlockTimm", [128, 768, 384]]
    - [-1, 6, "NormFreeBlockTimm", [768, 768, 384]]
    - [-1, 1, VarEMA]
      # stage 4
    - [-1, 1, "pt.modules.BlurPool", 768]
    - [-1, 1, VarEMA]
    - [-1, 5, "NormFreeBlockTimm", [768, 768, 384]]
      # head
    - [-1, 1, scaled_conv1x1, [768, 2304], {gamma: "${init_gamma}"}]
    - [-1, 1, 'torch.nn.SiLU']
    - [-1, 1, "pt.modules.FastGlobalAvgPool2d", [], {flatten: True}]
    - [-1, 1, "torch.nn.Dropout", [0.2]]
    - [-1, 1, "nn.Linear", [2304, 1000]]

# using best found optimizer so far
optim:
  _target_: pytorch_tools.optim.adamw.AdamW
  weight_decay: 5e-3
  eps: 1e-6
  lr: 0

# use label smoothing
criterion:
  smoothing: 0.1

loader:
  image_size: 224
  batch_size: 224
  blur_prob: 0.2
  gray_prob: 0.2
  color_twist_prob: 0.4
  re_prob: 0.3

val_loader:
  image_size: 288 # to test on the same resolution as timm

run:
  stages:
    # no warm-up
    - {start: 0, end: 90, lr: [0.005, 0], lr_mode: cos}

  extra_callbacks:
    - _target_: src.callbacks.CutmixMixup
      cutmix_alpha: 1.0
      mixup_alpha: 0.2
      prob: 1.0
      
filter_from_wd: [gain] # filter bias and gain from wd
init_gamma: 2 # larger value to compensate for variance reduction after BlurPool