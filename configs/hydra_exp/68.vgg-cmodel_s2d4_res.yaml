# @package _global_

# VGG using CModel
# + HardSwish 
# + AvgPool2d instead of MaxPool2d
# + space2depth block size 4
# + post-normalization
# + all convs are wrapped in residual blocks

# blyat' doesn't improve anything compared to model without residuals

# run2. 
# + add orthogonal initialization
# + add last BN
# + move pre-last BN from pre GAP to post-GAP
# upd. slightly better convergence on val, but still very bad :c

# run3.
# revert run2 changes
# + head width -> 2048 (like in ResNets) 
# + dropout 0.2
# gives 22M params
# adding last BN again leads to very large variance and NaNs

defaults:
  - /base@_here_
  
log:
  exp_name: vgg-cmodel_sd4_res_narrow-last-bn
  histogram: True
  print_model: True

model:
  _target_: sota_imagenet.model.CModel
  extra_kwargs:
    ReLU:
      inplace: True

  layer_config:
    # VGG16: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
    # 1
    - [-1, 1, pt.modules.SpaceToDepth, 4]
    - [-1, 1, ConvResidual, [conv3x3, 48, 64]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.BatchNorm2d, 64]
    # 2
    - [-1, 1, ConvResidual, [conv3x3, 64, 64]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.BatchNorm2d, 64]
    # - [-1, 1, nn.AvgPool2d, [2, 2]] # os=2
    # 3
    - [-1, 1, ConvResidual, [conv3x3, 64, 128]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.BatchNorm2d, 128]
    # 4`
    - [-1, 1, ConvResidual, [conv3x3, 128, 128]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.BatchNorm2d, 128]
    # - [-1, 1, nn.AvgPool2d, [2, 2]] # os=4
    # 5
    - [-1, 1, ConvResidual, [conv3x3, 128, 256]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.BatchNorm2d, 256]
    # 6
    - [-1, 1, ConvResidual, [conv3x3, 256, 256]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.BatchNorm2d, 256]
    # 7
    - [-1, 1, ConvResidual, [conv3x3, 256, 256]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.BatchNorm2d, 256]
    - [-1, 1, nn.AvgPool2d, [2, 2]] # os=8
    # 8
    - [-1, 1, ConvResidual, [conv3x3, 256, 512]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.BatchNorm2d, 512]
    # 9
    - [-1, 1, ConvResidual, [conv3x3, 512, 512]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.BatchNorm2d, 512]
    # 10
    - [-1, 1, ConvResidual, [conv3x3, 512, 512]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.BatchNorm2d, 512]
    - [-1, 1, nn.AvgPool2d, [2, 2]] # os=16
    # 11
    - [-1, 1, ConvResidual, [conv3x3, 512, 512]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.BatchNorm2d, 512]
    # 12
    - [-1, 1, ConvResidual, [conv3x3, 512, 512]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.BatchNorm2d, 512]
    # 13
    - [-1, 1, ConvResidual, [conv3x3, 512, 512]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.BatchNorm2d, 512]
    # in original VGG they have FC(512 * 7 * 7 -> 4096) here but i'm using GAP + FC(512 -> 4096)
    - [-1, 1, pt.modules.FastGlobalAvgPool2d, [], {flatten: True}]
    # - [-1, 1, nn.BatchNorm1d, 512]
    - [-1, 1, nn.Linear, [512, 2048]]
    - [-1, 1, nn.Hardswish]
    # - [-1, 1, nn.BatchNorm1d, 2048] # maybe replace this BN with FRN
    - [-1, 1, nn.Linear, [2048, 2048]]
    - [-1, 1, nn.Hardswish]
    - [-1, 1, nn.Dropout, 0.2]
    - [-1, 1, nn.Linear, [2048, 1000]]

# using default SGD
optim:
  _target_: torch.optim._multi_tensor.SGD
  momentum: 0.9
  weight_decay: 3e-5
  lr: 0

# use label smoothing
criterion:
  smoothing: 0.1

loader:
  image_size: 224
  batch_size: 224
  blur_prob: 0.2
  gray_prob: 0.2
  color_twist_prob: 0.4
  re_prob: 0.3

val_loader:
  image_size: 288 # to test on the same resolution as timm

run:
  stages:
    # SGD requires warmup
    - {start: 0, end: 3, lr: [0.001, 0.1]}
    - {start: 3, end: 90, lr: [0.1, 0], lr_mode: cos}

  extra_callbacks:
    - _target_: sota_imagenet.callbacks.CutmixMixup
      cutmix_alpha: 1.0
      mixup_alpha: 0.2
      prob: 1.0
    # - _target_: sota_imagenet.callbacks.OrthoInitClb
    #   gain: ${init_gamma}

filter_from_wd: [gain] # filter bias and gain from wd
# leaving gain param to apply proper init for the model
init_gamma: 1.7 # value from NFNet paper for SiLU activation
