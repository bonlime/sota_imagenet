# @package _global_

# config close to one in Nvidia/DeepLearning examples but with adamw

# with SGD it should be Acc@1: 77.1380 | Acc@5: 93.5880

# Results are on-par with results in AdamP and Novograd Papers. It works but is worse than SGD
# [07-02 08:31:49] - Train loss: 1.7580 | Acc@1: 81.6711 | Acc@5: 93.8324
# [07-02 08:31:49] - Val   loss: 1.8826 | Acc@1: 76.5640 | Acc@5: 93.2160
# [07-02 08:31:49] - Epoch 90: best loss improved from 1.8827 to 1.8826

# upd. changed AdamW to use eps inside sqrt. Let's try once again but with higher LR
# + eps in sqrt (change in code not in config). It means that eps=1e-6 is the same as eps=1e-3 before
# + increase lr to account for change in adaptivity
# + proper initialization of weight

defaults:
  - /base@_here_

model:
  _target_: pytorch_tools.models.resnet50

log:
  # exp_name: r50_adamw_fixed-eps
  exp_name: r50_adamw_fixed-small-eps
  histogram: True



# first run of this experiment
# optim:
#   _target_: torch.optim._multi_tensor.AdamW
#   weight_decay: 5e-2
#   lr: 0

# use smaller weight decay because of label smoothing and increased lr
# optim:
#   _target_: pytorch_tools.optim.adamw.AdamW
#   weight_decay: 1e-2
#   eps: 1e-6
#   lr: 0

# maybe right initialization is enough by itself and we can use smaller eps + same lr
optim:
  _target_: pytorch_tools.optim.adamw.AdamW
  weight_decay: 0.1
  eps: 1e-10
  lr: 0



# use label smoothing
criterion:
  smoothing: 0.1

loader:
  image_size: 224
  batch_size: 256

run:
  stages:
    - {start: 0, end: 8, lr: [0, 0.001]}
    - {start: 8, end: 90, lr: [0.001, 0], lr_mode: cos}

# run:
#   stages:
#     - {start: 0, end: 8, lr: [0, 0.01]}
#     - {start: 8, end: 90, lr: [0.01, 0], lr_mode: cos}

init_gamma: 1.7 # proper weight init
