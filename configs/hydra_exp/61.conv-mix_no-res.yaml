# @package _global_

# moving from NFNet to pure VGG-like architecture. different from exp60 here we don't even have residual.
# lets call this VGG strikes back
# conv -> act -> norm (?)

# No bias and gain in conv, using affine in BN for now. There was a paper which showed that it's easier to optimize
# pre-bias than post-bias (but they used ReLU activation maybe because of that it was better)
# it at least trains, but test performance is garbage in another experiment

# run2. bias and gain in conv + VarEMA. need to check after ~5 epochs how train performance compares to BN
# upd. val loss greatly increases and is very volatile which is pretty strange :c
# can't say that anything works yet

defaults:
  - /base@_here_
  
log:
  # exp_name: nf-conv-act_repeat
  exp_name: vgg-sb_zero_var-ema #_adaml
  histogram: True
  print_model: True

model:
  _target_: sota_imagenet.model.CModel
  extra_kwargs:
    VGGBlock:
      activation: "'swish_hard'"
      conv_kwargs:
        gamma: "${init_gamma}"
        gain_init: 1
        n_heads: 1
      # this conv kwargs are used with BN + affine
      # conv_kwargs:
      #   gamma: "${init_gamma}"
      #   gain_init: null
      #   bias: False
      #   n_heads: 1
    VarEMA:
      use: False
    scaled_conv3x3: {gamma: "${init_gamma}"}
    torch.nn.SiLU: {inplace: True}

  layer_config:
    # residual starts from the very firt layer. 
    # larger gain during first channel increase for faster convergence during training
    - [-1, 1, pt.modules.SpaceToDepth, 8]
    - [-1, 1, scaled_conv1x1, [192, 128], {gamma: "${init_gamma}"}] # 8 * 8 * 3 = 192
    - [-1, 1, torch.nn.Hardswish]
      # stage 1
    - [-1, 4, VGGBlock, [128, 128], {pre_norm: VarEMA(128)}]
    - [-1, 1, VGGBlock, [128, 384], {pre_norm: VarEMA(128)}]
    # - [-1, 4, VGGBlock, [128, 128], {pre_norm: "nn.BatchNorm2d(128, affine=True)"}]
    # - [-1, 1, VGGBlock, [128, 384], {pre_norm: "nn.BatchNorm2d(128, affine=True)"}]
    - [-1, 1, VarEMA, 384]
    - [-1, 1, nn.AvgPool2d, [2, 2]] # OS=16
      # stage 2
    # - [-1, 12, VGGBlock, [384, 384], {pre_norm: "nn.BatchNorm2d(384, affine=True)", groups_width: 64}]
    - [-1, 12, VGGBlock, [384, 384], {pre_norm: VarEMA(384), groups_width: 64}]
    - [-1, 1, VarEMA, 384]
      # stage head
    - [-1, 1, scaled_conv1x1, [384, 2304], {gamma: "${init_gamma}"}]
    - [-1, 1, torch.nn.Hardswish]
    - [-1, 1, pt.modules.FastGlobalAvgPool2d, [], {flatten: True}]
    - [-1, 1, torch.nn.Dropout, [0.1]]
    - [-1, 1, nn.Linear, [2304, 1000]]

# using default SGD

optim:
  _target_: torch.optim._multi_tensor.SGD
  momentum: 0.9
  weight_decay: 3e-5
  lr: 0

# optim:
#   # in PyTorch implementation weight decay is lr * wd, so 0.1 == 1e-4 as in AdamW paper
#   _target_: src.optimizers.AdamLayerwise
#   weight_decay: 1e-2
#   betas: [0.9, 0.995]
#   lr: 0

# use label smoothing
criterion:
  smoothing: 0.1

loader:
  image_size: 224
  batch_size: 224
  blur_prob: 0.2
  gray_prob: 0.2
  color_twist_prob: 0.4
  re_prob: 0.3

val_loader:
  image_size: 288 # to test on the same resolution as timm

run:
  stages:
    # SGD requires warmup
    # - {start: 0, end: 3, lr: [0.001, 0.1]}
    # - {start: 3, end: 90, lr: [0.1, 0], lr_mode: cos}

    - {start: 0, end: 90, lr: [0.1, 0], lr_mode: cos}


  extra_callbacks:
    - _target_: sota_imagenet.callbacks.CutmixMixup
      cutmix_alpha: 1.0
      mixup_alpha: 0.2
      prob: 1.0

filter_from_wd: [gain] # filter bias and gain from wd
init_gamma: 1.7 # value from NFNet paper for SiLU activation
