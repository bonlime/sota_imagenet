# @package _global_

# my version of nfnet model with shuffled grouped convolutions. only 8kk params compared to 24kk in timm
# training is same as in exp15 for comparison

# first run. it trains but 8M params probably requires much longer training to converge
# Train loss: 2.7577 | Acc@1: 66.3802 | Acc@5: 84.6261
# Val   loss: 2.1792 | Acc@1: 71.3480 | Acc@5: 89.5040

# run2.
# + remove gain from wd
# + heads=4. it will still allow merging all params together after training
# [07-10 02:06:13] - Train loss: 2.4537 | Acc@1: 72.2111 | Acc@5: 88.6808
# [07-10 02:06:13] - Val   loss: 1.9893 | Acc@1: 74.9260 | Acc@5: 92.0180

# run3. 
# same as run3 of exp15 (resume from previous + all possible augs) 

defaults:
  - /base@_here_

model:
  _target_: src.model.CModel
  extra_kwargs:
    NormFreeBlock: {activation: "'swish_hard'", groups_width: 64, alpha: 0.2, gamma: "${init_gamma}", n_heads: 4}

  layer_config:
    - [-1, 1, "pt.modules.SpaceToDepth", 2]  # 0
    - [-1, 1, "scaled_conv3x3", [12, 64], {gamma: "${init_gamma}"}]  # 0
    - [-1, 1, 'torch.nn.Hardswish', [], {inplace: True}]
      # stage 1
    - [-1, 1, "pt.modules.BlurPool", 64]
    - [-1, 1, "NormFreeBlock", [64, 128]]
      # stage 2
    - [-1, 1, "pt.modules.BlurPool", 128]
    - [-1, 1, "NormFreeBlock", [128, 192]]
    - [-1, 1, "NormFreeBlock", [192, 192]]
      # stage 3
    - [-1, 1, "pt.modules.BlurPool", 192]
    - [-1, 1, "NormFreeBlock", [192, 256], {groups_width: 48}]
    - [-1, 5, "NormFreeBlock", [256, 256]]
      # stage 4
    - [-1, 1, "pt.modules.BlurPool", 256]
    - [-1, 1, "NormFreeBlock", [256, 384]]
    - [-1, 4, "NormFreeBlock", [384, 384]]
      # head
    - [-1, 1, "ScaledStdConv2d", [384, 2560, 1], {padding: 1, gamma: "${init_gamma}"}]
    - [-1, 1, 'torch.nn.Hardswish', [], {inplace: True}]
    - [-1, 1, "pt.modules.FastGlobalAvgPool2d", [], {flatten: True}]
    - [-1, 1, "nn.Linear", [2560, 1000]]

log:
  exp_name: nf-cnet_more-heads_very-long
  histogram: True

# using best found optimizer so far
optim:
  _target_: pytorch_tools.optim.adamw.AdamW
  weight_decay: 1e-3
  eps: 1e-6
  lr: 0

# use label smoothing
criterion:
  smoothing: 0.1

loader:
  image_size: 224
  batch_size: 256
  blur_prob: 0.2
  gray_prob: 0.2
  color_twist_prob: 0.4
  re_prob: 0.3
  random_interpolation: True

val_loader:
  # image_size: 224  # for better comparison with R50 runs test on the same resolution. could get slightly better results by testing on 
  image_size: 288 # to test on the same resolution as timm

run:
  resume: logs/2021-07-08_nf-cnet_more-heads/11-23/model.chpn

  stages:
    - {start: 0, end: 5, lr: [0, 0.01]}
    - {start: 5, end: 360, lr: [0.01, 0], lr_mode: cos}

  accumulate_steps: 2  
  ema_decay: 0.9997 
  extra_callbacks:
    - _target_: src.callbacks.CutmixMixup
      cutmix_alpha: 1.0
      mixup_alpha: 0.2
      prob: 1 # always perform mixup or cutmix
      
  # extra_callbacks:
  #   - _target_: pytorch_tools.fit_wrapper.callbacks.Cutmix
  #     alpha: 1.0
  #     # not needed really because now OHE is in loader, but needed for legacy reasons
  #     num_classes: 1000
  #     prob: 0.5

    # not needed for rwightman model, because it uses StdConvs by default
    # - _target_: src.callbacks.ForwardWeightNorm
    #   gamma: "${init_gamma}"
    #   use_std: True

filter_from_wd: [gain] # filter bias and gain from wd
init_gamma: 2 # larger value to compensate for variance reduction after BlurPool